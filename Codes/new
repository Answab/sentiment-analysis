{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2477ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c6f431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           OriginalTweet\n",
      "0      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...\n",
      "1      advice Talk to your neighbours family to excha...\n",
      "2      Coronavirus Australia: Woolworths to give elde...\n",
      "3      My food stock is not the only one which is emp...\n",
      "4      Me, ready to go at supermarket during the #COV...\n",
      "...                                                  ...\n",
      "44950  Meanwhile In A Supermarket in Israel -- People...\n",
      "44951  Did you panic buy a lot of non-perishable item...\n",
      "44952  Asst Prof of Economics @cconces was on @NBCPhi...\n",
      "44953  Gov need to do somethings instead of biar je r...\n",
      "44954  I and @ForestandPaper members are committed to...\n",
      "\n",
      "[44955 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# importing dataset\n",
    "\n",
    "dataset = pd.read_csv(r\"C:\\Users\\ANSWEB\\Desktop\\Major Project\\Dataset\\TwitterDataset.csv\",\n",
    "    usecols=['OriginalTweet'], encoding='latin-1')\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89e30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining tokenization function\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# punctuations without '@' and '#'\n",
    "punctuation_pattern = '[!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~]'\n",
    "\n",
    "def text_clean(x) :\n",
    "    # remove newline and carriage return\n",
    "    x = x.replace('\\n', ' ').replace('\\r', '')\n",
    "    # remove the links\n",
    "    x = nltk.re.sub(r'http:\\S+|https:\\S+|www\\S+', '', x)\n",
    "    # remove punctuation marks and emoticons \n",
    "    x = nltk.re.sub(punctuation_pattern, '', x)\n",
    "    words = x.split(' ')\n",
    "    temp = []\n",
    "    \n",
    "    for word in words :\n",
    "        word=word.lower()\n",
    "        if (word not in stop_words) and (not word[1:].isnumeric()) and (not word.startswith('@')):\n",
    "            if word.startswith('#'):\n",
    "                temp.append(word[1:])\n",
    "            else:\n",
    "                temp.append(word)\n",
    "                \n",
    "    pattern = '[!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~@#\\$]'\n",
    "    cleaned_sentence = nltk.re.sub(pattern, '', ' '.join(temp))\n",
    "    \n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4799c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying tokenization on dataset\n",
    "\n",
    "dataset['CleanTweet'] = dataset['OriginalTweet'].apply(lambda x : text_clean(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e944a39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'she', \"you'll\", 'theirs', 'against', 'below', 'on', \"hadn't\", \"mightn't\", 'some', 'now', 'y', 'what', 'about', \"weren't\", 'themselves', 'ours', 'through', 'your', 'i', 'where', 'will', 'how', 'up', 'off', 'you', 'ma', 'who', 'after', 'not', 'have', 'just', 'had', 's', \"couldn't\", 'same', 'why', 'once', 'ain', 'did', 've', \"you've\", 'because', 'hasn', 'of', 'shouldn', 'having', 'before', 'in', 'over', 'when', 'didn', 'then', 'by', 'down', 'itself', 'has', 'o', \"that'll\", 'they', 'their', 'if', \"didn't\", 'mustn', 'until', 'which', 'few', 'am', \"haven't\", \"shan't\", 'as', 'll', 'my', 'more', \"mustn't\", 'been', 'and', 'them', 'wouldn', 'most', 'wasn', 'haven', 'needn', 'me', 'being', 'those', 'he', 'him', 'mightn', 'do', 'into', 'himself', 'but', \"doesn't\", 'very', 'such', 'so', 'that', 'above', 'hadn', 'the', 'only', 'aren', 'whom', 'shan', 'weren', 'it', \"aren't\", 't', 'is', 'are', 'at', 'under', 'both', \"isn't\", 'again', 'during', 'were', 'further', 'own', 'here', 'can', \"it's\", 'to', 'd', 'we', 'be', \"wouldn't\", \"needn't\", 'all', 'was', 'yourself', 'each', \"you're\", 'his', 'won', 'isn', \"don't\", 'doing', 'yours', \"she's\", 'yourselves', \"shouldn't\", 'out', 'herself', 'from', 'any', 'couldn', 're', 'between', \"hasn't\", 'our', \"wasn't\", 'for', 'this', \"won't\", 'or', 'there', 'hers', \"you'd\", 'don', 'with', 'while', 'than', 'does', 'doesn', 'these', 'her', 'should', \"should've\", 'an', 'no', 'm', 'too', 'a', 'myself', 'ourselves', 'other', 'nor', 'its'}\n"
     ]
    }
   ],
   "source": [
    "# displaying stop words\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af6bfa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                         \n",
       "1        advice talk neighbours family exchange phone n...\n",
       "2        coronavirus australia woolworths give elderly ...\n",
       "3        food stock one empty  please dont panic enough...\n",
       "4        ready go supermarket covid19 outbreak  im para...\n",
       "                               ...                        \n",
       "44950    meanwhile supermarket israel  people dance sin...\n",
       "44951    panic buy lot nonperishable items echo needs f...\n",
       "44952    asst prof economics talking recent research co...\n",
       "44953    gov need somethings instead biar je rakyat ass...\n",
       "44954    members committed safety employees endusers mo...\n",
       "Name: CleanTweet, Length: 44955, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying tokenized dataset\n",
    "\n",
    "dataset['CleanTweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83957f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the lemmatization function\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemm(text):\n",
    "\n",
    "    # tokenizing text into words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # lemmatizing each word\n",
    "    lemmatized_words = []\n",
    "    for token in tokens:\n",
    "        # getting the part-of-speech (POS)\n",
    "        pos_tag = nltk.pos_tag([token])[0][1]\n",
    "        pos = wordnet.VERB if pos_tag.startswith('V') else wordnet.NOUN if pos_tag.startswith('N') else wordnet.ADJ if pos_tag.startswith('J') else wordnet.ADV\n",
    "        lemma = lemmatizer.lemmatize(token, pos=pos)\n",
    "        lemmatized_words.append(lemma)\n",
    "\n",
    "    # joining lemmatized words back into a sentence\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39ac973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying lemmatization on dataset\n",
    "\n",
    "dataset['Lemmatized']=dataset.CleanTweet.apply(lambda x : lemm(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "230979c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                         \n",
       "1        advice talk neighbour family exchange phone nu...\n",
       "2        coronavirus australia woolworth give elderly d...\n",
       "3        food stock one empty please dont panic enough ...\n",
       "4        ready go supermarket covid19 outbreak im paran...\n",
       "                               ...                        \n",
       "44950    meanwhile supermarket israel people dance sing...\n",
       "44951    panic buy lot nonperishable item echo need foo...\n",
       "44952    asst prof economics talk recent research coron...\n",
       "44953    gov need somethings instead biar je rakyat ass...\n",
       "44954    member commit safety employee endusers monitor...\n",
       "Name: Lemmatized, Length: 44955, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying lemmatized dataset\n",
    "\n",
    "dataset['Lemmatized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b6f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving lemmatized dataset\n",
    "\n",
    "dataset.to_csv('LemmatizedData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0853e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining vectorization function \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=4999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "257ed83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying vectorization\n",
    "\n",
    "vectors = vectorizer.fit_transform(dataset[\"Lemmatized\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "TfIdf = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64a814eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the tfidf model\n",
    "\n",
    "joblib.dump(vectorizer, 'TFIDF.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c4e60dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving vectorized dataset\n",
    "\n",
    "TfIdf.to_csv('TFIDFData.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
